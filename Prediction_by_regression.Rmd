---
title: "Predicting the net hourly electrical energy output of a combined cycle power plant "
author: "Tesfahun Tegene Boshe, Cynara Nyahoda"
date: "5/22/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: show
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```
# Introduction
In this article we will predict the net hourly electrical energy output of a combined cycle power plant using four different regression models. The dataset contains 36733 instances of 11 sensor measures aggregated over one hour (by means of average or sum) from a gas turbine located in Turkey's north western region for the purpose of studying flue gas emissions, namely CO and NOx (NO + NO2). We believe the dataset can be well used for predicting turbine energy yield (TEY) using ambient variables as features.We will discuss the data preparation, variable selection, modeling and testing the models in that order. The models will later be compared using well known metrics.

# Data Preparation
## Install and load the necessary packages
```{r}

requiredPackages = c("janitor","verification","olsrr","DescTools","caret","tibble","purrr","corrplot","corrplot","dbplyr","dplyr","readr", "ggplot2", "glmnet")

for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE)}

```  

```{r, echo=FALSE}
setwd("~/sem 2/Machine Learning 1/project/regression/pp_gas_emission")
```
Load the train data.
The source data is available [Here](https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set)   
The description of variables is as below.

![Data description](./image.jpg)

```{r}
data_emissions <- read.csv("gas_train.csv", header = TRUE)
head(data_emissions,6) # the top 6 rows
```

```{r}

str(data_emissions)
```

## Data preparation

Below we will do some exploratory data analysis, hoping to get the general picture of the data. 

### Finding the missing entries
Variables in columns 67 to 92 are to be removed for the same reason as above. 
```{r}
# remove more columns
sum(is.na(data_emissions))
```

Our data has no missing values, therefore imputation won't be needed. 

### Finding the outliers {.tabset}
Just like the missing values, it is important to check and handle the outliers appropriately for the better model accuracy. 

Let's start with analyzing basic statistics. 
```{r}

summary(data_emissions)
# glimpse(data_emissions)
```

Box plots of the variables can indicate the outliers if any. 

```{r}
library(reshape)
meltData <- melt(data_emissions)
p <- ggplot(meltData, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")

```
As we can see, variables TIT, CO and NOX have outliers. We will remove the instances with those values in the next stage. 

#### TIT outliers
```{r, echo = T, results = 'hide'}
outlier_TIT <- boxplot(data_emissions$TIT,data = data_emissions)$out
```

#### CO outliers
```{r, echo = T, results = 'hide'}
outlier_CO <- boxplot(data_emissions$CO,data = data_emissions)$out
```

#### NOX outliers
```{r, echo = T, results = 'hide'}

outlier_NOX <- boxplot(data_emissions$NOX,data = data_emissions)$out
```

Remove the outliers. 
```{r}
data_emissions_clean <- data_emissions[!((data_emissions$TIT %in% outlier_TIT) |(data_emissions$CO %in% outlier_CO) |(data_emissions$NOX %in% outlier_NOX)),]

```

### Distribution of the dependent variable

Now that we have cleaned our data, it is time to take a closer look at the dependent variable, *Turbine Energy Yield (TEY) *. Let's plot it first. 

```{r}
# distribution of the dependent variable:- TEY
ggplot(data_emissions,
       aes(x = TEY)) +
  geom_histogram(fill = "blue",
                 bins = 100) +
  theme_bw()

```

> The dependent variable his more or less symmetric, making the variable easier to model. , Therefore,there is no need of log transformation.  

# Variable selection

## Omit the variables with 1 level.
Variables experiencing no change across the observations should be excluded from the model. Let's find out if we have any such variable. 
```{r}

sapply(data_emissions, 
       function(x) 
         unique(x) %>% 
         length()) %>% 
  sort()
```

Another way to check this is: 
```{r}
vars_selected <-names(data_emissions)
var_to_remove <- nearZeroVar(data_emissions, 
                                      names = TRUE)

var_to_remove
```

> We keep all variables because none has a zero/near zero variance. 

## Remove the collinear (strongly correlated) independent variables
Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation). The interpretation of effect of individual predictor variables are not reliable when multicolinearity is present. It is important, therefore, to exclude them from the model.
```{r}

data_emissions_correlations <- cor(data_emissions,
    use = "pairwise.complete.obs")

data_emissions_cor_order <- 
  data_emissions_correlations[,"TEY"] %>% 
  sort(decreasing = TRUE) %>% # sort the correlations with TEY in decreasing order
  names()

```

Plotting the correlations
```{r}
# using the 30 most correlated variables
corrplot.mixed(data_emissions_correlations[data_emissions_cor_order, 
                                   data_emissions_cor_order],
               upper = "square",
               lower = "number",
               tl.col = "black",
               tl.pos = "lt")

```

We see that there are variables highly correlated to one another. Next step should be finding the potential candidates to be excluded from the model

```{r}
vars_to_remove <- findCorrelation(data_emissions_correlations[-8,-8],
                cutoff = 0.9, # threshold
                names = TRUE)

vars_selected <- names(data_emissions)[
  !names(data_emissions) %in% vars_to_remove
]
```

```{r}
data_emissions <- data_emissions[,vars_selected]
```

> Variable "CDP" has been removed. 


## Omit variables which are linear combinations of one another.
Variables that are linear combinations of other variables should be removed. They do not add anything, but only make everything a little harder. 

```{r}

( findLinearCombos(data_emissions) ->
    data_emissions_linearCombos )

```
> We do not have variables that are linear combinations of each other.

## Insignificant variables
The last stage of variable selection is removing insignificant variables. While this can be done by stepwise variable selection,  an automated Backward elimination can make the task less manual. 
 
1. Run the model with all variables
```{r}


modelWithallVars <- lm(TEY ~ .,
                          data = data_emissions %>%
                            dplyr::select(all_of(vars_selected))) #


```

2. Remove variables with p value less than 5%
```{r}

Model_varsRemoved <- ols_step_backward_p(modelWithallVars,
                    prem = 0.05, # p-value threshold
                    progress = FALSE) # hide progress

```

3. Remove the recommended variables
```{r}

vars_to_remove <- Model_varsRemoved$removed
vars_selected <- vars_selected[!vars_selected %in% vars_to_remove]

```
> Our variables are all significant, none to be removed. 

# Training the model
Once the training and cross validation steps are done, we will need to test our models. It is a good time to load the test data. 

Import the test data. 
```{r}
data_emissions_train <- data_emissions # train data from the previous steps. 
data_emissions_test <- read.csv("gas_test.csv", header = TRUE)
```

## Methods {.tabset}
Regression can be implemented in various ways. We have used the following 4 models, and their results are to be compared. 
1. Linear Regression
2. Ridge Regression
3. Lasso Regression
4. Elastic Net Regression

### Simple Linear Regression
Linear regression algorithm works by selecting coefficients for each independent variable that minimizes a loss function
```{r}

Model_SimpleRegr <- lm(TEY ~ ., 
                  data = data_emissions_train %>% 
                   dplyr::select(all_of(vars_selected))) # training the model

summary(Model_SimpleRegr)
```

### Ridge Regression
Ridge regression is an extension of linear regression where the loss function is modified by adding a penalty parameter to minimize the complexity of the model.
Ridge regression is a regularization technique, a procedure of penalizing large coefficients to avoid overfitting on the training dataset.

Create model matrix of the train data
```{r}
x_train = model.matrix(TEY~., data_emissions_train)[,-1] # trim off the first column
                                         # leaving only the predictors
y_train = data_emissions_train$TEY%>%
  unlist() %>%
  as.numeric()
```

Training the ridge model
```{r}
grid = 10^seq(10, -2, length = 100)
Model_ridge <- glmnet(x_train, y_train, alpha = 0, lambda = grid) # alpha = o for ridge model

summary(Model_ridge)
```

### Lasso Regression
Lasso regression model is similar to Ridge regression except that the alpha parameter is set to 1. 

```{r}

Model_lasso <- glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   lambda = grid) # Fit lasso model on training data

summary(Model_lasso)
```

### Elastic Net Regression
Elastic net regression combines the properties of ridge and lasso regression. It works by penalizing the model using both the 1l2-norm1 and the 1l1-norm1. 

```{r}

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 5,
                              search = "random",
                              verboseIter = FALSE)

# Train the model
Model_ENR <- train(TEY ~ .,
                           data = data_emissions_train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
Model_ENR$bestTune
```

## Model validation
- Cross validation determines the expected forecast error before looking into the test data. - It can estimate the skill of the model on *unseen data*. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model. 

Finding the lambda parameter that minimizes the mean square error. 

a. Ridge model
```{r}
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 0) # Fit ridge regression model
bestlam = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam
```

a. Lasso model
```{r}
set.seed(1)
cv.out = cv.glmnet(x_train, y_train, alpha = 1) # Fit ridge regression model
bestlam2 = cv.out$lambda.min  # Select lamda that minimizes training MSE
bestlam2
```
# Testing the model on out of sample data

## Preparing the test data.
The dataset comes in train and test sets. It is therefore necessary to apply all the data cleaning processes on the test data before applying model prediction. 

Check for missing values
```{r}
sum(is.na(data_emissions_test)) # any missing value? 

```

Remove the outliers. 
```{r}

data_emissions_test <- data_emissions_test[!((data_emissions_test$TIT %in% outlier_TIT) |(data_emissions_test$CO %in% outlier_CO) |(data_emissions_test$NOX %in% outlier_NOX)),]
```

Remove the same variables removed from the train data. 
```{r}
data_emissions_test <- data_emissions_test[,vars_selected]
```

Prepare the test data
```{r}
x_test = model.matrix(TEY~., data_emissions_test)[,-1] # trim off the first column
                                         # leaving only the predictors
y_test = data_emissions_test$TEY%>%
  unlist() %>%
  as.numeric()
```

## Prediction  

Let's find the prediction results for each model. 
```{r}

Model_SimpleRegrfitted <- predict(Model_SimpleRegr,
                               data_emissions_test)
Model_ridgefitted <- predict(Model_ridge, s = bestlam, newx = x_test)

Model_lassofitted <- predict(Model_lasso, s = bestlam2, newx = x_test)
Model_ENRfitted <- predict(Model_ENR, x_test)

```

# Accuracy measurement

## Visual error distribution
Let's check the histograms of residuals for each model. 
```{r}
res1 <- data.frame(residual = Model_SimpleRegrfitted - y_test)
res2 <- data.frame(residual = Model_ridgefitted - y_test) 
res3 <- data.frame(residual = Model_lassofitted - y_test)
res4 <- data.frame(residual = Model_ENRfitted - y_test)

res1$model <- 'm1';res2$model <- 'm2';res3$model <- 'm3';res4$model <- 'm4'
colnames(res2) <- c("residual","model")
colnames(res3) <- c("residual","model")
DF_residuals <- rbind(res1, res2,res3,res4)


ggplot(DF_residuals, aes(residual, fill = model)) + geom_density(alpha = 0.2)

```

## Using correlation
The best model is the one with the largest coorelation coefficient with the true values. 

```{r}

DF <- data.frame(real = y_test,predicted_1 = Model_SimpleRegrfitted,
                 predicted_2 = Model_ridgefitted, predicted_3 = Model_lassofitted,
                 predicted_4 = Model_ENRfitted,time = 1:length(y_test))
colnames(DF) <- c("real","predicted_1","predicted_2","predicted_3","predicted_4","time")

```

```{r}
cor(DF[,-c(1,6)],
    DF$real)

```

> All predictions are strongly correlated with the real/true values. 

## Using statistical metrics
Some of the most commonly used statistical metrics are MSE: mean square error, RMSE: root mean square error, MAE: Mean Absolute Error, R^2:  R squared and others. The "regressionMetrics" function calculates these and other metrics. 

```{r}

regressionMetrics <- function(real, predicted) {
  MSE <- mean((real - predicted)^2)  # Mean Squera Error
  RMSE <- sqrt(MSE) # Root Mean Square Error
  MAE <- mean(abs(real - predicted)) # Mean Absolute Error
  MAPE <- mean(abs(real - predicted)/real) # Mean Absolute Percentage Error
  MedAE <- median(abs(real - predicted)) # Median Absolute Error
  MSLE <- mean((log(1 + real) - log(1 + predicted))^2) # Mean Logarithmic Absolute Error
  TSS <- sum((real - mean(real))^2) # Total Sum of Squares
  RSS <- sum((predicted - real)^2) # Explained Sum of Squares
  R2 <- 1 - RSS/TSS  # R squared
  
  result <- data.frame(MSE, RMSE, MAE, MAPE, MedAE, MSLE, R2)
  return(result)
}

```

Let's call the function above on all combinations of real, predicted values. 

```{r}
rm_1 <- regressionMetrics(DF$real,
                  predicted = DF$predicted_1) # model 1
rm_2 <- regressionMetrics(DF$real,
                  DF$predicted_2) # model 2
rm_3 <- regressionMetrics(DF$real,
                  DF$predicted_3) # model 3 
rm_4 <- regressionMetrics(DF$real,
                  DF$predicted_4) # model 4

```

TO compare the results in a table,  
```{r}
metrics <- rbind(rm_1,rm_2,rm_3,rm_4)
row.names(metrics) <- c("Model_SimpleRegr","Model_ridge","Model_lasso","Model_ENR")
metrics
```

# Conclusion
Ridge regression and lasso regression with selected parameters achieve better predicting accuracy than a simple regression model. Elastic Net Regression improves the model accuracy even further. 

